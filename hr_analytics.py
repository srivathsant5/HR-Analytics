# -*- coding: utf-8 -*-
"""HR Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Qv0qj_JWLzCiVUXlWlhw-3aFfuj7rXW

# ***HR Analytics***

# ***Problem Statement***

Your client is a large MNC and they have 9 broad verticals across the organisation. One of the problem your client is facing is around identifying the right people for promotion (only for manager position and below) and prepare them in time. Currently the process, they are following is:



1. They first identify a set of employees based on recommendations/ past performance


2. Selected employees go through the separate training and evaluation program for each vertical. These programs are based on the required skill of each vertical


3. At the end of the program, based on various factors such as training performance, KPI completion (only employees with KPIs completed greater than 60% are considered) etc., employee gets promotion
"""

# Commented out IPython magic to ensure Python compatibility.
#import Neccessory libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split, GridSearchCV

#import required accuracy metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.model_selection import KFold, cross_val_score

import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

#loading the data set
df = pd.read_csv(r"/content/train.csv")
df.head(5)

dft = pd.read_csv(r"/content/test.csv")
dft.head(2)

ss = pd.read_csv(r"/content/sample_submission_M0L0uXE.csv")
ss.head(2)

#lets check the shape 
print('Shape of train dataset:',df.shape)

#check the data types
df.dtypes

#lets check for Null Values
df.isnull().sum()

#Lets replace null values from all missing  numerical columns with the median value of that column

df["previous_year_rating"].fillna(df["previous_year_rating"].median(),inplace=True)

#Lets replace null values from  all missing  Categorical columns with the mean value of that column

df['education'] = df['education'].fillna(df['education'].mode()[0])

#lets check for Null Values

df.isnull().sum()

#Lets check which columns contains '?'
df[df.columns[(df == '?').any()]].nunique()

#Lets chcek the value counts for categorical data
for i in df.columns:
    if df[i].dtypes == 'object':
        print(df[i].value_counts())
        print('---------'*30)

df.dtypes

for col in df.columns:
    print(col,df[col].nunique())
    print('-'*35)

"""# ***`EDA`***"""

#lets check distribution for continuous columns
num_data = df._get_numeric_data()
plt.figure(figsize = (10,25))
plotnumber = 1
for column in num_data:
    if plotnumber <=10:
        ax = plt.subplot(17,5,plotnumber)
        sns.distplot(num_data[column])
        plt.xlabel(column,fontsize = 10)
    plotnumber+=1
plt.tight_layout()

"""## ***Check Hit Map to find Corelation ***"""

#Lets plot heatmap to check correlation among differnt features and label

df_corr = df.corr()
plt.figure(figsize = (10,5))
sns.heatmap(df_corr,vmin=-1,vmax=1,annot=True,center=0,fmt='.2g',linewidths=0.1)
plt.tight_layout()

import plotly.express as px

st_con_vt=df[['gender','previous_year_rating','department','is_promoted']]
fig = px.sunburst(st_con_vt, path=['gender','previous_year_rating','department','is_promoted'], values='is_promoted',
                  color='department',
                 color_continuous_scale='viridis_r')
fig.update_layout(title_text='Sunburst Image with gender, department and region')
fig.show()

#lets check relation between gender and Loan_Status
sns.countplot(x = 'gender', hue = 'is_promoted', data = df)
plt.show()

#lets check the fraud_report based on age
plt.figure(figsize = (8,5))
sns.countplot(x = 'department', hue = 'is_promoted', data = df)
plt.show()

#lets check the fraud_report based on age
plt.figure(figsize = (8,5))
sns.countplot(x = 'education', hue = 'is_promoted', data = df)
plt.show()

#lets check the fraud_report based on age
plt.figure(figsize = (8,5))
sns.countplot(x = 'KPIs_met >80%', hue = 'is_promoted', data = df)
plt.show()

#lets check the fraud_report based on age
plt.figure(figsize = (8,5))
sns.countplot(x = 'awards_won?', hue = 'is_promoted', data = df)
plt.show()

#lets check the fraud_report based on age
plt.figure(figsize = (12,5))
sns.countplot(x = 'length_of_service', data = df)
plt.show()

#lets check the fraud_report based on age
plt.figure(figsize = (12,5))
sns.countplot(x = 'age', data = df)
plt.show()

#lets check counts for Education
plt.style.use('fivethirtyeight')
sns.countplot(x = 'is_promoted', data = df)
plt.show()

#check the relation between column Education and Loan Status
sns.countplot(x = 'no_of_trainings', hue = 'is_promoted', data = df)
plt.show()

#lets plot barplot for MonthlyCharges vs churn
sns.barplot(x = 'is_promoted', y = 'avg_training_score', data = df)
plt.show()

#lets plot barplot for MonthlyCharges vs churn
sns.barplot(x = 'is_promoted', y = 'previous_year_rating', data = df)
plt.show()

#lets check the relation between Dependents, TotalCharges, churn using Violin plot

plt.style.use('default')
sns.catplot(x="department", y="avg_training_score", hue="is_promoted",
            kind="violin", split=True,
            palette="pastel", data=df)
plt.show()

#lets check the relation between Dependents, TotalCharges, churn using Violin plot
plt.style.use('default')
sns.catplot(x="gender", y="previous_year_rating", hue="is_promoted",
            kind="violin", split=True,
            palette="pastel", data=df)
plt.show()

#lets check the relation between Dependents, TotalCharges, churn using Violin plot
plt.style.use('default')
sns.catplot(x="gender", y="age", hue="is_promoted",
            kind="violin", split=True,
            palette="pastel", data=df)
plt.show()

#lets have a look at destribution of Loan_Amount_Term
sns.distplot(df['age'])
plt.show()

#lets have a look at destribution of Loan_Amount_Term
sns.distplot(df['avg_training_score'])
plt.show()

#Lets have a look on loan_status_relationship, how it is distributed

plt.figure(figsize=(20, 15))
plt.pie( df["region"].value_counts().values, labels = df["region"].value_counts().index, autopct='%1.1f%%')
fig = plt.gcf()
plt.title('region')
plt.axis('equal')
plt.legend(prop={'size': 20})
plt.legend( bbox_to_anchor = (1.05, 1), loc = 'upper left')
plt.show()

#Lets have a look on loan_status_relationship, how it is distributed

plt.figure(figsize=(10, 6))
plt.pie( df["department"].value_counts().values, labels = df["department"].value_counts().index, autopct='%1.1f%%')
fig = plt.gcf()
plt.title('department')
plt.axis('equal')
plt.legend(prop={'size': 20})
plt.legend( bbox_to_anchor = (1.05, 1), loc = 'upper left')
plt.show()

#Lets have a look on loan_status_relationship, how it is distributed

plt.figure(figsize=(5, 3))
plt.pie( df["education"].value_counts().values, labels = df["education"].value_counts().index, autopct='%1.1f%%')
fig = plt.gcf()
plt.title('education')
plt.axis('equal')
plt.legend(prop={'size': 20})
plt.legend( bbox_to_anchor = (1.05, 1), loc = 'upper left')
plt.show()

"""## **Pre-Processing**"""

#lets describe the data
df.describe().T

"""## `***Checking for outliers using box plots***`"""

df.dtypes

#Lets have a look on distribution of our data
cols=['no_of_trainings','age','previous_year_rating','length_of_service','KPIs_met >80%','awards_won?','avg_training_score','is_promoted']
plt.style.use('default')
plt.figure(figsize = (10,15))
plotnumber = 1
for column in cols:
    if plotnumber <=15:
        ax = plt.subplot(4,3,plotnumber)
        sns.boxenplot(df[column], color="red")
        #plt.title(f"Distribution of {column}",fontsize=10)
        plt.xlabel(column,fontsize = 10)
    plotnumber+=1
plt.tight_layout()

"""# get dummies"""

df= pd.get_dummies(df, columns=['department','education','gender','recruitment_channel','previous_year_rating','KPIs_met >80%','awards_won?'],drop_first=True)

df.nunique()

df.dtypes

df.isna().sum()

df.head(6)

display(df.drop_duplicates())

"""## **Split Data into x & y**"""

#lets saperate data into label and features
x = df.drop(columns = 'is_promoted')
y = df["is_promoted"]

x.skew()

#Lets treat the skewness

for index in x.skew().index:
    if x.skew().loc[index]>0.5:
        x[index]=np.log1p(x[index])
        if x.skew().loc[index]<-0.5:
            x[index]=np.square(x[index])

x.skew()

num_data = x.select_dtypes(include = [np.number])
cat_data = x.select_dtypes(exclude=[np.number])

num_data.head(3)

cat_data.head(3)

"""## Applying standard scaler to numerical data"""

#Lets bring all numerical features to common scale by applying standard scaler

scaler = StandardScaler()
num = scaler.fit_transform(num_data)
num = pd.DataFrame(num,columns=num_data.columns)

num.head(4)

"""## Encoding"""

from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
for i in cat_data.columns:
    cat_data[i] = enc.fit_transform(cat_data[i].values.reshape(-1,1))

cat_data.head(4)

"""## **combining categorical and numerical data**"""

X = pd.concat([num, cat_data], axis = 1)

X.head(6)

#check the shape
X.shape

#check value count for target variable
y.value_counts()

"""# Over sampling"""

#lets do oversampling using SMOTE
import imblearn
from imblearn.over_sampling import SMOTE
SM = SMOTE()
x_over,y_over = SM.fit_resample(X,y)

#lets check the count of target variable now
y_over.value_counts()

"""## **Finding best random_state**"""

#Lets find the best random state using LogisticRegression
from sklearn.linear_model import LogisticRegression
max_accu = 0
max_rs = 0
for i in range(50,100):
    x_train,x_test,y_train,y_test = train_test_split(x_over,y_over,test_size = 0.25, random_state = i)
    LR = LogisticRegression()
    LR.fit(x_train,y_train)
    pred = LR.predict(x_test)
    acc = accuracy_score(y_test,pred)
    if acc > max_accu:
        max_accu = acc
        max_rs = i
print("Best accuracy is",max_accu,"on Random State",max_rs)

#lets split our data into train and test parts with best random_state
x_train,x_test,y_train,y_test = train_test_split(x_over, y_over, test_size = 0.25, random_state = 93)

"""## **Model Building with Evaluation Metrics**

# **LogisticRegression model**
"""

#Lets check the model with LogisticRegression
LR.fit(x_train,y_train)
predlr = LR.predict(x_test)
accuracy = accuracy_score(y_test,predlr)*100

print(f"Accuracy Score:", accuracy)
print(f"roc_auc_score: {roc_auc_score(y_test,predlr)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,predlr)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,predlr)}")

#cross validation score
scores = cross_val_score(LR, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\nAccuracy Score - Cross Validation Score :", result)

"""## **DecisionTreeClassifier model**"""

#model with DecesionTreeClassifier
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(x_train,y_train)
pred_dt = dt.predict(x_test)
accuracy = accuracy_score(y_test,pred_dt)*100

print(f"Accuracy Score:", accuracy)
print(f"roc_auc_score: {roc_auc_score(y_test,pred_dt)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred_dt)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred_dt)}")

#cross validation score
scores = cross_val_score(dt, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\n\nAccuracy Score - Cross Validation Score :", result)

"""## RandomForestClassifier model"""

#model with RandomForestClassifier


from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(x_train,y_train)
pred_rf = model.predict(x_test)
accuracy = accuracy_score(y_test,pred_rf)*100

print(f"Accuracy Score:", accuracy)
print(f"\nroc_auc_score: {roc_auc_score(y_test,pred_rf)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred_rf)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred_rf)}")

#cross validation score
scores = cross_val_score(model, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\n\nAccuracy Score - Cross Validation Score :", result)

"""## **KNeighborsClassifier model**"""

#model with KNeighborsClassifier


from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
kn.fit(x_train,y_train)
pred_kn = kn.predict(x_test)
accuracy = accuracy_score(y_test,pred_kn)*100

print(f"Accuracy Score:", accuracy)
print(f"roc_auc_score: {roc_auc_score(y_test,pred_kn)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred_kn)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred_kn)}")

#cross validation score
scores = cross_val_score(kn, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\n\nAccuracy Score - Cross Validation Score :", result)

"""## **XGBClassifier model**"""

#lets check with XGBClassifier model
from xgboost import XGBClassifier
xgb = XGBClassifier(verbosity = 0)
xgb.fit(x_train,y_train)
pred_xgb = xgb.predict(x_test)
accuracy = accuracy_score(y_test,pred_xgb)*100

print(f"Accuracy Score:", accuracy)
print(f"roc_auc_score: {roc_auc_score(y_test,pred_xgb)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred_xgb)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred_xgb)}")

#cross validation score
scores = cross_val_score(xgb, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\nAccuracy Score - Cross Validation Score :", result)

"""## **ExtraTreesClassifier model**"""

#lets check with Extra Trees Classifier


from sklearn.ensemble import ExtraTreesClassifier
ext = ExtraTreesClassifier()
ext.fit(x_train,y_train)
pred_ext = xgb.predict(x_test)
accuracy = accuracy_score(y_test,pred_ext)*100

print(f"Accuracy Score:", accuracy)
print(f"roc_auc_score: {roc_auc_score(y_test,pred_ext)*100}")
print("---------------------------------------------------")

#confusion matrix & classification report
print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred_ext)}\n")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred_ext)}")

#cross validation score
scores = cross_val_score(ext, x_over, y_over, cv = 5,scoring = "accuracy" ).mean()*100
print("\nCross validation score :", scores)

#result of accuracy minus cv score
result = accuracy - scores
print("\nAccuracy Score - Cross Validation Score :", result)

"""## AUC & ROC Curve"""

#Lets plot roc curve and check auc and performance of all algorithms


from sklearn.metrics import plot_roc_curve
disp = plot_roc_curve(LR, x_test, y_test)
plot_roc_curve(dt, x_test, y_test, ax = disp.ax_)
plot_roc_curve(rf, x_test, y_test, ax = disp.ax_)
plot_roc_curve(kn, x_test, y_test, ax = disp.ax_)
plot_roc_curve(xgb, x_test, y_test, ax = disp.ax_)
plot_roc_curve(ext, x_test, y_test, ax = disp.ax_)
plt.figure(figsize = (25,25))
plt.show()

"""## **Hyperparameter Tuning**"""

#lets selects different parameters for tuning


grid_params = {
               'criterion':['gini','entropy'],
                'max_depth': [10,12,15,20,22],
                'n_estimators':[500,700,1000,1200],
                'max_features':['aoto','sqrt','log2'],
                'min_samples_split': [2]
                }

#train the model with given parameters using GridSearchCV


#GCV =  GridSearchCV(RandomForestClassifier(), grid_params, cv = 5)
#GCV.fit(x_train,y_train)

#GCV.best_params_ 

#printing the best parameters found by GridSearchCV

#lets check the results of final model with best parameters


model = RandomForestClassifier(criterion = 'gini', max_depth = 22, min_samples_split = 2,  n_estimators = 1200)
model.fit(x_train,y_train)
pred = model.predict(x_test)

print(f"Accuracy Score: {accuracy_score(y_test,pred)*100}%")
print("-----------------------------------------------------------------------")

print(f"roc_auc_score: {roc_auc_score(y_test,pred)*100}%")
print("------------------------------------------------------------------------")

print(f"Confusion Matrix : \n {confusion_matrix(y_test,pred)}\n")
print("------------------------------------------------------------------------")
print(f"CLASSIFICATION REPORT : \n {classification_report(y_test,pred)}")

"""## **AUC ROC CURVE for final model**"""

plot_roc_curve(model, x_test, y_test)
plt.title('ROC Curve for best model')
plt.show()

"""# ***Test_Data_Set***"""

test = pd.read_csv('/content/test.csv')
test.head(5)

L_ID = test['employee_id']
#test = test.drop(columns='employee_id')

test.info()

test.isna().sum()

"""filling null values"""

#Lets replace null values from all missing  numerical columns with the median value of that column

test["previous_year_rating"].fillna(test["previous_year_rating"].median(),inplace=True)

#Lets replace null values from  all missing  Categorical columns with the mean value of that column

test['education'] = test['education'].fillna(test['education'].mode()[0])

test.isna().sum()

"""***get dummies***"""

test= pd.get_dummies(test, columns=['department','education','gender','recruitment_channel','previous_year_rating','KPIs_met >80%','awards_won?'],drop_first=True)

test.skew()

#Lets treat the skewness

for index in test.skew().index:
    if test.skew().loc[index]>0.5:
        test[index]=np.log1p(test[index])
        if test.skew().loc[index]<-0.5:
            test[index]=np.square(test[index])

test.skew()

num_data2 = test.select_dtypes(include = [np.number])
cat_data2 = test.select_dtypes(exclude=[np.number])

#Lets bring all numerical features to common scale by applying standard scaler

scaler = StandardScaler()
num2 = scaler.fit_transform(num_data2)
num2 = pd.DataFrame(num2,columns=num_data2.columns)

from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
for i in cat_data2.columns:
    cat_data2[i] = enc.fit_transform(cat_data2[i].values.reshape(-1,1))

test = pd.concat([num2, cat_data2], axis = 1)

test.shape

X.shape

test.columns

X.columns

#lets predict the price with our best model
prediction = model.predict(test)

prediction

#lets make the dataframe for prediction
hr = pd.DataFrame(prediction, columns=["is_promoted"])

HR_file = pd.concat([L_ID, hr], axis = 1)

#Lets save the submission to csv

HR_file.to_csv("HR_ Analysis.csv",index=False)

HR_file.head(5)

HR_file.shape

ss.shape

"""## `***THANK YOU***`"""